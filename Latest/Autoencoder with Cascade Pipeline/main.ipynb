{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D,UpSampling2D,concatenate\n",
    "from keras.layers import BatchNormalization, ReLU, Activation, Concatenate, Conv2DTranspose, Reshape,Softmax\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "random.seed(165)\n",
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH =\"/home/azka/KLProjectCombined/Knee/KneeXrayData/ClsKLData/kneeKL224\"\n",
    "IMSIZE =128, 128\n",
    "\n",
    "\n",
    "def normalize(img):\n",
    "    mat = np.zeros(img.shape, dtype=np.int16)\n",
    "    h, w = img.shape[:2]\n",
    "    m1 = img.min()\n",
    "    m2 = img.max()\n",
    "    for j in range(h):\n",
    "        for i in range(w):\n",
    "            mat[j, i] = ((img[j, i] - m1) / (m2 - m1)) * 255\n",
    "    return mat\n",
    "\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def find_joint(img):\n",
    "    cols = img.sum(axis = 1)\n",
    "    cols = np.array(cols,dtype='int64')\n",
    "    slope = []\n",
    "    for i in range(100, len(cols)-1):\n",
    "        slope.append(float(cols[i+1] - cols[i]))\n",
    "    j = np.argmin(slope) + 100\n",
    "    return j\n",
    "\n",
    "def Class5Label(lab): \n",
    "\n",
    "    lb = [0] *5\n",
    "    lb[int(lab)] = 1\n",
    "    return lb\n",
    "\n",
    "\n",
    "def Class2Label(label): \n",
    "    if label=='2' or label=='1' or label=='0':\n",
    "        lab='0'\n",
    "    elif label=='3' or label=='4':\n",
    "        lab='1'\n",
    "\n",
    "    lb = [0] *2\n",
    "    lb[int(lab)] = 1\n",
    "    return lb\n",
    "\n",
    "def DataLoad(imdir):\n",
    "    folders = glob.glob(os.path.join(imdir,'*'))\n",
    "    img_a = []\n",
    "    img_b = []\n",
    "    img_c = []\n",
    "    lb = []\n",
    "    lb=[]\n",
    "    cls = []\n",
    "    lr = []\n",
    "    lb5=[]\n",
    "    for folder in folders:\n",
    "        impath = glob.glob(os.path.join(folder,'*.png'))\n",
    "        label = folder[-1]\n",
    "        print('Loading', len(impath), label, 'images ...', 'HOHO')\n",
    "        for i, p in enumerate(impath):\n",
    "            img_whole = cv2.imread(p,0)\n",
    "            h, w = img_whole.shape\n",
    "            if impath[0][-5] == 'L':\n",
    "                im_a = img_whole\n",
    "                img_a.append(im_a)\n",
    "                img_b.append(im_a)\n",
    "                lb.append(Class2Label(label))\n",
    "                cls.append(int(label))\n",
    "                lr.append([1,0])\n",
    "                lb5.append(Class5Label(label))\n",
    "\n",
    "            elif impath[0][-5] == 'R':\n",
    "                im_b = img_whole\n",
    "                img_whole = cv2.flip(img_whole, 1)\n",
    "                im_a = img_whole\n",
    "                img_a.append(im_a)\n",
    "                img_b.append(im_b)\n",
    "                #img_c.append(im_c)\n",
    "                lb.append(Class2Label(label))\n",
    "                cls.append(int(label))\n",
    "                lr.append([0,1])\n",
    "                lb5.append(Class5Label(label))\n",
    "    return img_a, img_b, img_c, lb, np.array(cls), lr, lb5\n",
    "\n",
    "\n",
    "def ImagePreprocessing(img):\n",
    "    h, w = IMSIZE\n",
    "    print('Preprocessing ...')\n",
    "    for i, im, in tqdm(enumerate(img)):\n",
    "        tmp = cv2.resize(im, dsize=(w, h), interpolation=cv2.INTER_AREA)\n",
    "        tmp = NormalizeData(cv2.equalizeHist(tmp))\n",
    "        img[i] = tmp\n",
    "    print(len(img), 'images processed!')\n",
    "    return img\n",
    "\n",
    "imageA, imageB, _, labels,classes, lr,lb5 = DataLoad(os.path.join(DATASET_PATH, 'train'))\n",
    "imageA = ImagePreprocessing(imageA)\n",
    "imageA = np.array(imageA)\n",
    "imageA = np.expand_dims(imageA, axis=-1)\n",
    "\n",
    "imageB = ImagePreprocessing(imageB)\n",
    "imageB = np.array(imageB)\n",
    "imageB = np.expand_dims(imageB, axis=-1)\n",
    "\n",
    "lb5=np.array(lb5)\n",
    "labels = np.array(labels)\n",
    "lr = np.array(lr)\n",
    "\n",
    "X_train = imageA\n",
    "X_trainLR = imageB\n",
    "Y_train = labels\n",
    "Y_train2=lb5\n",
    "Y_trainLR=lr\n",
    "\n",
    "imageA, imageB, _, labels,classes, lr,lb5 = DataLoad(os.path.join(DATASET_PATH, 'val'))\n",
    "imageA = ImagePreprocessing(imageA)\n",
    "imageA = np.array(imageA)\n",
    "imageA = np.expand_dims(imageA, axis=-1)\n",
    "\n",
    "imageB = ImagePreprocessing(imageB)\n",
    "imageB = np.array(imageB)\n",
    "imageB = np.expand_dims(imageB, axis=-1)\n",
    "\n",
    "labels = np.array(labels)\n",
    "lr = np.array(lr)\n",
    "lb5=np.array(lb5)\n",
    "\n",
    "#labelsL = np.array(labelsL)\n",
    "#labelsR = np.array(labelsR)\n",
    "lr = np.array(lr)\n",
    "X_val = imageA\n",
    "X_valLR = imageB\n",
    "Y_val = labels\n",
    "Y_val2=lb5\n",
    "Y_valLR=lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageA, imageB, _, labels,classes, lr,lb5 = DataLoad(os.path.join(DATASET_PATH, 'test'))\n",
    "imageA = ImagePreprocessing(imageA)\n",
    "imageA = np.array(imageA)\n",
    "imageA = np.expand_dims(imageA, axis=-1)\n",
    "\n",
    "imageB = ImagePreprocessing(imageB)\n",
    "imageB = np.array(imageB)\n",
    "imageB = np.expand_dims(imageB, axis=-1)\n",
    "\n",
    "labels = np.array(labels)\n",
    "lr = np.array(lr)\n",
    "lb5=np.array(lb5)\n",
    "\n",
    "X_test = imageA\n",
    "\n",
    "X_testLR = imageB\n",
    "Y_test = labels\n",
    "Y_test2=lb5\n",
    "Y_testLR=lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for sub-classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Class2Label(label): \n",
    "    if label=='3':\n",
    "        lab='0'\n",
    "    elif label=='4':\n",
    "        lab='1'\n",
    "\n",
    "    lb = [0] *2\n",
    "    lb[int(lab)] = 1\n",
    "    return lb\n",
    "\n",
    "IMSIZE=128,128\n",
    "def DataLoad34(imdir):\n",
    "    folders = glob.glob(os.path.join(imdir,'*'))\n",
    "    img_a = []\n",
    "    img_b = []\n",
    "    img_c = []\n",
    "    lb = []\n",
    "    lb=[]\n",
    "    cls = []\n",
    "    lr = []\n",
    "    lb5=[]\n",
    "    for folder in folders:\n",
    "        impath = glob.glob(os.path.join(folder,'*.png'))\n",
    "        label = folder[-1]\n",
    "        if label=='3' or label=='4':\n",
    "            print('Loading', len(impath), label, 'images ...', 'HOHO')\n",
    "            for i, p in enumerate(impath):\n",
    "                img_whole = cv2.imread(p,0)\n",
    "                h, w = img_whole.shape\n",
    "                if impath[0][-5] == 'L':\n",
    "                    im_a = img_whole\n",
    "                    #im_b = img_whole\n",
    "                    #im_c = img_whole\n",
    "                    img_a.append(im_a)\n",
    "                    #img_b.append(im_b)\n",
    "                    #img_c.append(im_c)\n",
    "                    lb.append(Class2Label(label))\n",
    "                    cls.append(int(label))\n",
    "                    lr.append(0)\n",
    "                    lb5.append(Class5Label(label))\n",
    "\n",
    "                elif impath[0][-5] == 'R':\n",
    "                    img_whole = cv2.flip(img_whole, 1)\n",
    "                    im_a = img_whole\n",
    "                    im_b = img_whole\n",
    "                    #im_c = img_whole\n",
    "                    img_a.append(im_a)\n",
    "                    img_b.append(im_b)\n",
    "                    #img_c.append(im_c)\n",
    "                    lb.append(Class2Label(label))\n",
    "                    cls.append(int(label))\n",
    "                    lr.append(1)\n",
    "                    lb5.append(Class5Label(label))\n",
    "    return img_a, img_b, img_c, lb, np.array(cls), lr, lb5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "imageA, imageB, _, labels,classes, lr,_ = DataLoad34(os.path.join(DATASET_PATH, 'train'))\n",
    "imageA = ImagePreprocessing(imageA)\n",
    "imageA = np.array(imageA)\n",
    "imageA = np.expand_dims(imageA, axis=-1)\n",
    "labels = np.array(labels)\n",
    "lr = np.array(lr)\n",
    "\n",
    "X_train34 = imageA\n",
    "Y_train34 = labels\n",
    "imageA, imageB, _, labels,classes, lr,_ = DataLoad34(os.path.join(DATASET_PATH, 'val'))\n",
    "imageA = ImagePreprocessing(imageA)\n",
    "imageA = np.array(imageA)\n",
    "imageA = np.expand_dims(imageA, axis=-1)\n",
    "\n",
    "labels = np.array(labels)\n",
    "lr = np.array(lr)\n",
    "lb5=np.array(lb5)\n",
    "#labelsL = np.array(labelsL)\n",
    "#labelsR = np.array(labelsR)\n",
    "lr = np.array(lr)\n",
    "X_val34 = imageA\n",
    "Y_val34 = labels\n",
    "\n",
    "\n",
    "imageA, imageB, _, labels,classes, lr,_ = DataLoad34(os.path.join(DATASET_PATH, 'test'))\n",
    "imageA = ImagePreprocessing(imageA)\n",
    "imageA = np.array(imageA)\n",
    "imageA = np.expand_dims(imageA, axis=-1)\n",
    "\n",
    "labels = np.array(labels)\n",
    "lr = np.array(lr)\n",
    "\n",
    "lb5=np.array(lb5)\n",
    "\n",
    "X_test34 = imageA\n",
    "Y_test34 = labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Class2Label(lab): \n",
    "    lb = [0] *3\n",
    "    lb[int(lab)] = 1\n",
    "    return lb\n",
    "\n",
    "#IMSIZE=512,512\n",
    "\n",
    "def DataLoad012(imdir):\n",
    "    folders = glob.glob(os.path.join(imdir,'*'))\n",
    "    img_a = []\n",
    "    img_b = []\n",
    "    img_c = []\n",
    "    lb = []\n",
    "    lb=[]\n",
    "    cls = []\n",
    "    lr = []\n",
    "    lb5=[]\n",
    "    for folder in folders:\n",
    "        impath = glob.glob(os.path.join(folder,'*.png'))\n",
    "        label = folder[-1]\n",
    "        if label=='0' or label=='1' or label=='2':\n",
    "            print('Loading', len(impath), label, 'images ...', 'HOHO')\n",
    "            for i, p in enumerate(impath):\n",
    "                img_whole = cv2.imread(p,0)\n",
    "                h, w = img_whole.shape\n",
    "                if impath[0][-5] == 'L':\n",
    "                    im_a = img_whole\n",
    "                    img_a.append(im_a)\n",
    "                    lb.append(Class2Label(label))\n",
    "                    cls.append(int(label))\n",
    "                    lr.append(0)\n",
    "\n",
    "                elif impath[0][-5] == 'R':\n",
    "                    im_b = img_whole\n",
    "                    img_whole = cv2.flip(img_whole, 1)\n",
    "                    im_a = img_whole\n",
    "                    img_a.append(im_a)\n",
    "                    \n",
    "                    lb.append(Class2Label(label))\n",
    "                    cls.append(int(label))\n",
    "                    lr.append(1)\n",
    "    return img_a, lb, np.array(cls), lr, lb5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "imageA,labels,classes, lr,_ = DataLoad012(os.path.join(DATASET_PATH, 'train'))\n",
    "imageA = ImagePreprocessing(imageA)\n",
    "imageA = np.array(imageA)\n",
    "imageA = np.expand_dims(imageA, axis=-1)\n",
    "labels = np.array(labels)\n",
    "lr = np.array(lr)\n",
    "\n",
    "X_train012 = imageA\n",
    "Y_train012 = labels\n",
    "\n",
    "\n",
    "imageA, labels,classes, lr,_ = DataLoad012(os.path.join(DATASET_PATH, 'val'))\n",
    "imageA = ImagePreprocessing(imageA)\n",
    "imageA = np.array(imageA)\n",
    "imageA = np.expand_dims(imageA, axis=-1)\n",
    "\n",
    "labels = np.array(labels)\n",
    "lr = np.array(lr)\n",
    "lb5=np.array(lb5)\n",
    "lr = np.array(lr)\n",
    "X_val012 = imageA\n",
    "Y_val012 = labels\n",
    "\n",
    "\n",
    "imageA,labels,classes, lr,_ = DataLoad012(os.path.join(DATASET_PATH, 'test'))\n",
    "imageA = ImagePreprocessing(imageA)\n",
    "imageA = np.array(imageA)\n",
    "imageA = np.expand_dims(imageA, axis=-1)\n",
    "\n",
    "labels = np.array(labels)\n",
    "lr = np.array(lr)\n",
    "\n",
    "X_test012 = imageA\n",
    "Y_test012 = labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_img):    \n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small and thick)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv4 = BatchNormalization(name='encoder')(conv4)\n",
    "    return conv4\n",
    "\n",
    "def decoder(conv4):    \n",
    "    #decoder\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) #7 x 7 x 128\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) #7 x 7 x 64\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same',name='decoder')(up2) # 28 x 28 x 1\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Latent rep from Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(x):\n",
    "    x= Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x= Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2), padding = 'same')(x) \n",
    "    x= Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x= Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2), padding = 'same')(x) \n",
    "    x= Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x= Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2), padding = 'same')(x) \n",
    "    x= Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x= Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2), padding = 'same')(x) \n",
    "    x= Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x= Conv2D(512, (3, 3), padding='same')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2), padding = 'same')(x) \n",
    "    x= Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x= Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    flatten=Flatten()(x)\n",
    "    x = Dense(1000,activation='relu')(flatten)# for multiclass classifier i used this\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    x = Dense(64,activation='relu')(x)\n",
    "    x = Dense(2)(x)\n",
    "    x = Softmax(name='classifier')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoencoder():\n",
    "    input_img = Input(shape=(128,128,1),name=\"input\")\n",
    "    encode = encoder(input_img)\n",
    "    decode=decoder(encode)\n",
    "    classifier=fc(encode)\n",
    "    \n",
    "    model = Model(inputs=[input_img],outputs=[classifier,decode])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoNet = get_autoencoder()\n",
    "autoNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1(y, y_hat):\n",
    "    bce=tf.keras.losses.BinaryCrossentropy()\n",
    "    bce=bce(y,y_hat)\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    tn = tf.reduce_sum((1 - y_hat) * (1 - y), axis=0)\n",
    "    \n",
    "    specificity = tn / (tn + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "    return 0.5*bce+1-(0.8*recall+0.2*specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD \n",
    "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "sgd = SGD(0.0005, momentum=0.9, nesterov=False)\n",
    "m = tf.keras.metrics.FalseNegatives()\n",
    "acc = tf.keras.metrics.Accuracy()\n",
    "logs_base_dir = \"./logs\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logs_base_dir, histogram_freq=1)\n",
    "bce=tf.keras.losses.BinaryCrossentropy()\n",
    "autoNet.compile(optimizer=optimizer,loss=[macro_f1,'mse'],loss_weights=[1,0.3],metrics=['accuracy'])\n",
    "checkpoint_filepath='/home/azka/My KL Grade Project/Notebooks/modelmain'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_classifier_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "csv_logger = CSVLogger('base_classifier.csv', separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set(font_scale=2)\n",
    "pred=autoNet.predict([X_test])\n",
    "pred = pred[0].argmax(axis=1)\n",
    "actual = Y_test.argmax(axis=1)\n",
    "accuracy = (pred == actual).sum()/len(pred)\n",
    "cm=confusion_matrix(actual,pred)\n",
    "sns.heatmap(cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training binary classifier for grade 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subclassifier():\n",
    "    input_img = Input(shape=(128,128,1),name=\"input\")\n",
    "    encode = encoder(input_img)\n",
    "    decode=decoder(encode)\n",
    "    classifier=fc(encode)\n",
    "    model = Model(inputs=[input_img],outputs=[classifier,decode])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binClassifier = get_subclassifier()\n",
    "binClassifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1,l2 in zip(binClassifier.layers[:17],autoNet.layers[0:17]):\n",
    "    l1.set_weights(l2.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in binClassifier.layers[0:17]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD \n",
    "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "sgd = SGD(0.0005, momentum=0.9, nesterov=False)\n",
    "m = tf.keras.metrics.FalseNegatives()\n",
    "acc = tf.keras.metrics.Accuracy()\n",
    "bce=tf.keras.losses.BinaryCrossentropy()\n",
    "binClassifier.compile(optimizer=optimizer,loss=[macro_f1,'mse'],loss_weights=[1,0.3],metrics=['accuracy'])\n",
    "checkpoint_filepath='/home/azka/My KL Grade Project/Notebooks/model'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_classifier_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "csv_logger = CSVLogger('bin_classifier.csv', separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_train = binClassifier.fit([X_train34],[Y_train34,X_train34], batch_size=8,validation_data=([X_val34],[Y_val34,X_val34]),epochs=70,verbose=1,shuffle=True,callbacks=[csv_logger,model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "binClassifier.load_weights(checkpoint_filepath)\n",
    "classify_train = pd.read_csv('bin_classifier.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binClassifier.evaluate([X_train34],[Y_train34,X_train34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binClassifier.evaluate([X_val34],[Y_val34,X_val34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binClassifier.evaluate([X_test34],[Y_test34,X_test34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(classify_train['classifier_accuracy'])\n",
    "plt.plot(classify_train['val_classifier_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(classify_train['classifier_loss'])\n",
    "plt.plot(classify_train['val_classifier_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set(font_scale=2)\n",
    "pred=binClassifier.predict(X_test34)\n",
    "pred = pred[0].argmax(axis=1)\n",
    "actual = Y_test34.argmax(axis=1)\n",
    "accuracy = (pred == actual).sum()/len(pred)\n",
    "cm=confusion_matrix(actual,pred)\n",
    "sns.heatmap(cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade 0,1,2 classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_test012, Y_test012 = shuffle(X_test012, Y_test012)\n",
    "X_val012, Y_val012 = shuffle(X_val012, Y_val012)\n",
    "X_train012, Y_train012 = shuffle(X_train012, Y_train012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc1(x):\n",
    "    x= Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    x= Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2), padding = 'same')(x) \n",
    "    \n",
    "    x=Dropout(0.3)(x)\n",
    "    x= Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x= Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size = (2,2), padding = 'same')(x) \n",
    "    x= Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x= Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size = (2,2), padding = 'same')(x) \n",
    "    x= Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x= Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2), padding = 'same')(x) \n",
    "    x= Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x= Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x=Flatten()(x)\n",
    "    x = Dense(1000,activation='relu')(x)# for multiclass classifier i used this\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    x = Dense(64,activation='relu')(x)\n",
    "    x = Dense(3)(x)\n",
    "    x = Softmax(name='classifier')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subclassifier2():\n",
    "    input_img = Input(shape=(128,128,1),name=\"input\")\n",
    "    encode = encoder(input_img)\n",
    "    decode=decoder(encode)\n",
    "    classifier=fc1(encode)\n",
    "    model = Model(inputs=[input_img],outputs=[classifier,decode])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulClassifier = get_subclassifier2()\n",
    "mulClassifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1,l2 in zip(mulClassifier.layers[:17],autoNet.layers[0:17]):\n",
    "    l1.set_weights(l2.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in mulClassifier.layers[0:17]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD \n",
    "import shutil\n",
    "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "m = tf.keras.metrics.FalseNegatives()\n",
    "acc = tf.keras.metrics.Accuracy()\n",
    "logs_base_dir = \"./logs\"\n",
    "if os.path.exists(logs_base_dir):\n",
    "    shutil.rmtree(logs_base_dir)\n",
    "os.makedirs(logs_base_dir)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logs_base_dir, histogram_freq=1)\n",
    "cce=tf.keras.losses.CategoricalCrossentropy()\n",
    "kl=tf.keras.losses.KLDivergence()\n",
    "mulClassifier.compile(optimizer=optimizer,loss=[cce,'mse'],loss_weights=[1.,0.5],metrics=['accuracy'])\n",
    "checkpoint_filepath='/home/azka/My KL Grade Project/Notebooks/modelNormal'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_classifier_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "csv_logger = CSVLogger('mul_classifier.csv', separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train012.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_train = mulClassifier.fit([X_train012],[Y_train012,X_train012], batch_size=8,validation_data=([X_val012],[Y_val012,X_val012]),epochs=30,verbose=1,shuffle=True,callbacks=[csv_logger,model_checkpoint_callback])#,tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mulClassifier.load_weights(checkpoint_filepath)\n",
    "classify_train = pd.read_csv('mul_classifier.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulClassifier.evaluate([X_train012],[Y_train012,X_train012])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulClassifier.evaluate([X_val012],[Y_val012,X_val012])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulClassifier.evaluate([X_test012],[Y_test012,X_test012])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set(font_scale=2)\n",
    "pred=mulClassifier.predict(X_test012)\n",
    "pred = pred[0].argmax(axis=1)\n",
    "actual = Y_test012.argmax(axis=1)\n",
    "accuracy = (pred == actual).sum()/len(pred)\n",
    "cm=confusion_matrix(actual,pred)\n",
    "sns.heatmap(cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLGradeModel(keras.Model):\n",
    "    def __init__(self, autoNet, binClassifier, mulClassifier, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.autoNet = autoNet\n",
    "        self.binClassifier = binClassifier\n",
    "        self.mulClassifier = mulClassifier\n",
    "    def __call__(self, inputs):\n",
    "        yNormalSevere = self.autoNet(inputs)\n",
    "        y_s = tf.argmax(yNormalSevere[0],axis=1)==0\n",
    "        normal_images = tf.boolean_mask(inputs, y_s)\n",
    "        severe_images = tf.boolean_mask(inputs, ~y_s)\n",
    "        Y_12 = self.mulClassifier(normal_images)\n",
    "        Y_34= self.binClassifier(severe_images)\n",
    "        return yNormalSevere,y_s,Y_12, Y_34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=KLGradeModel(autoNet,binClassifier,mulClassifier)\n",
    "yNormalSevere,y_s,Y_12, Y_34=model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predictions(yNS,Y_12, Y_34):\n",
    "    predictions=[]\n",
    "    x=0\n",
    "    y=0\n",
    "    y34=np.array(tf.argmax(Y_34,axis=1))\n",
    "    y12=np.array(tf.argmax(Y_12,axis=1))\n",
    "    y_s=np.array(tf.argmax(yNS,axis=1))\n",
    "    for s in y_s:\n",
    "        if s==0:    \n",
    "            if y12[x]==0:\n",
    "                predictions.append(0)\n",
    "            elif y12[x]==1:\n",
    "                predictions.append(1)\n",
    "                \n",
    "            elif y12[x]==2:\n",
    "                predictions.append(2)\n",
    "            x=x+1\n",
    "        if s==1:\n",
    "            if y34[y]==0:\n",
    "                predictions.append(3)\n",
    "            elif y34[y]==1:\n",
    "                predictions.append(4)\n",
    "\n",
    "            y=y+1\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=Predictions(yNormalSevere[0].numpy(),Y_12[0].numpy(), Y_34[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actual=Y_test2.argmax(axis=1)\n",
    "#Actual=np.where(Actual==0,1,Actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set(font_scale=2)\n",
    "accuracy = (preds == Actual).sum()/len(preds)\n",
    "cm=confusion_matrix(Actual,preds)\n",
    "sns.heatmap(cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0],'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train012[1560].squeeze(),'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=10\n",
    "h=10\n",
    "fig=plt.figure(figsize=(15, 15))\n",
    "columns = 5\n",
    "rows = 5\n",
    "for i in range(1, columns*rows +1):\n",
    "    img = np.random.randint(10, size=(h,w))\n",
    "    ax = fig.add_subplot(rows, columns, i)\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    plt.imshow(np.squeeze(X_train)[i], 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=10\n",
    "h=10\n",
    "fig=plt.figure(figsize=(15, 15))\n",
    "columns = 5\n",
    "rows = 5\n",
    "for i in range(1, columns*rows +1):\n",
    "    img = np.random.randint(10, size=(h,w))\n",
    "    ax = fig.add_subplot(rows, columns, i)\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    plt.imshow(np.squeeze(out_imgs)[i], 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
